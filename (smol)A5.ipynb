{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRBKNDIzf+fPfhpf0rLltF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdiTheRipper/DL-Lab/blob/main/Ass5(smol).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M8SXVrZhPgja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5295feff-ca5d-429d-e563-46140cfd2ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tLoss: 433.1032280921936\n",
            "\n",
            "Epoch: 2 \tLoss: 428.7868821620941\n",
            "\n",
            "Epoch: 3 \tLoss: 425.5364544391632\n",
            "\n",
            "Epoch: 4 \tLoss: 422.5021929740906\n",
            "\n",
            "Epoch: 5 \tLoss: 420.14970874786377\n",
            "\n",
            "(74, 100)\n",
            "(74, 74)\n",
            "{'deep': ['on', 'including', 'human', 'family', 'speech']}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Data preparation\n",
        "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
        "\"\"\"\n",
        "dl_data = data.split()\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(dl_data)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2\n",
        "\n",
        "# Generate training data\n",
        "def generate_context_word_pairs(corpus, window_size=2, vocab_size=None):\n",
        "    context_length = window_size * 2\n",
        "\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word = []\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "\n",
        "            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
        "            label_word.append(word)\n",
        "\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            y = to_categorical(label_word, num_classes=vocab_size)\n",
        "\n",
        "            yield (x, y)\n",
        "\n",
        "# Model building\n",
        "cbow = keras.models.Sequential()\n",
        "cbow.add(keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "cbow.add(keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "cbow.add(keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0.\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "        if i % 100000 == 0:\n",
        "            print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "    print()\n",
        "\n",
        "# Output\n",
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
        "                   for search_term in ['deep']}\n",
        "\n",
        "print(similar_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAGNfxczbpw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
